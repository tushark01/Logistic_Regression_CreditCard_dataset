# -*- coding: utf-8 -*-
"""Logistic_Regression_CreditCard_dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B38j3fPqdZBDmHUo9OSbrxlRrwAzG8VR
"""

# import the necessary packages
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset from the csv file using pandas
data = pd.read_csv('/content/sample_data/creditcard.csv')

data.info()

data.head()

data.describe()

##histogram
data.plot.hist(subplots=True, legend=True, layout=(11,3),figsize = (20, 20))
plt.savefig('histogram.png', dpi=300, bbox_inches='tight')
plt.show()

data['Class'].value_counts()

#Distribution of Values of Class
data["Class"].plot.hist(legend=True,figsize = (20,20))
plt.title('Distribution of Class')

Fraud = data[data['Class'] == 1]
Valid = data[data['Class'] == 0]

print('Fraud Cases: {}'.format(len(Fraud)))
print('Valid Transactions: {}'.format(len(Valid)))

#under-sampling
Valid_sample = Valid.sample(n=492)

new_df = pd.concat([Valid_sample, Fraud], axis=0)

new_df.shape

new_df['Class'].value_counts()

#Visualization of Balanced Distribution
new_df["Class"].plot.hist(legend=True,figsize = (5,5))
plt.title('Distribution of Class')

# Correlation matrix
corrmat = data.corr()
fig = plt.figure(figsize = (10,10))
sns.heatmap(corrmat,square = True,cmap="Blues")

plt.show()

#Creating X and Y for Training the Model
X = new_df.drop(columns="Class",axis=1)
y = new_df["Class"]

# Print shapes
print(X.shape)
print(y.shape)

#Train & test the dataset

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=2)
print(X.shape, X_train.shape, X_test.shape)

#Train the Logistic Regression Model

from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)

#Predict Using Trained Model

X_test_prediction = model.predict(X_test)
X_test_prediction

#Evaulate model performance using Confusion Matrix

from sklearn.metrics import confusion_matrix
data = confusion_matrix(y_test,X_test_prediction)
df_cm = pd.DataFrame(data, columns=np.unique(y_test), index = np.unique(y_test))
df_cm.index.name = 'Actual'
df_cm.columns.name = 'Predicted'

plt.figure(figsize = (5,5))
sns.set(font_scale=1.4)
plt.title('Confusion Matrix')
sns.heatmap(df_cm, annot=True,cmap="Blues")

plt.show()

#Classification Report

from sklearn.metrics import classification_report,accuracy_score
labels=["0","1"]
f, ax = plt.subplots(figsize=(5,5))
class_report=classification_report(y_test,X_test_prediction,target_names=labels, output_dict=True)
sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T, annot=True,ax=ax,cmap="Blues")
ax.set_title('Classification Report')

plt.show()

data_test_accuracy = accuracy_score(X_test_prediction, y_test)
data_test_accuracy

X_train_prediction = model.predict(X_train)
X_train_prediction

from sklearn.metrics import accuracy_score
data_train_accuracy = accuracy_score(X_train_prediction, y_train)
data_train_accuracy

"""## #Conclusion:

##Here by we can conclude that the model has not been over-fitted or under-fitted as the testing and training accuracies are almost in the same range. Therefore, we can conclude that the model performance is very efficient and accurate to deploy and use for predictions.
"""